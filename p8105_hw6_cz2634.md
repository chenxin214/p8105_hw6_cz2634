P8105 HW6
================
Chenxin Zhang
2020-12-02

## Problem 1

``` r
homicide_df =
  read_csv("./data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```

Start with one city.

``` r
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")
# fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. 
glm(resolution ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate), # odds ratio
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```

| term              |    OR | CI\_lower | CI\_upper |
| :---------------- | ----: | --------: | --------: |
| (Intercept)       | 1.363 |     0.975 |     1.907 |
| victim\_age       | 0.993 |     0.987 |     1.000 |
| victim\_raceWhite | 2.320 |     1.648 |     3.268 |
| victim\_sexMale   | 0.426 |     0.325 |     0.558 |

Try this across cities.

``` r
models_results_df =
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)
    ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate), # odds ratio
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI"))
```

``` r
models_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw6_cz2634_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" style="display: block; margin: auto;" />

## Problem 2

**Import and Tidy Data** *`babysex:` baby’s sex (male = 1, female = 2)
*`frace:` father’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto
Rican, 8 = Other, 9 = Unknown) *`mrace:` mother’s race (1 = White, 2 =
Black, 3 = Asian, 4 = Puerto Rican, 8 = Other) *`malform:` presence of
malformations that could affect weight (0 = absent, 1 = present)

``` r
#load draft data frame
#convert numeric to factor
#change pounds to grams
bw_df = read_csv("./data/birthweight.csv") %>% janitor::clean_names() %>% 
  mutate(
    babysex = factor(babysex, labels = c("male", "female")),
    frace = factor(frace, levels = c(1,2,3,4,8,9), labels = c("white", "black", "asian", "puerto rican", "other", "unknown")),
    mrace = factor(mrace, levels = c(1,2,3,4,8,9), labels = c("white", "black", "asian", "puerto rican", "other", "unknown")),
    malform = factor(malform, labels = c("absent", "present")),
  delwt = delwt * 453.59237,
    ppwt = ppwt * 453.59237,
    wtgain = wtgain * 453.59237) %>% 
  arrange(babysex, frace)
```

``` r
#there are two ways to check for missing data
sum(is.na(bw_df))
## [1] 0
which(is.na(bw_df))
## integer(0)
```

  - The dataset is consist of 4342 observations and 20 variables, and
    there is 0 missing value. The numeric variable `babysex`, `frace`,
    `mrace`, and `malform` are converted to factor variable.

**Propose a regression model for birthweight**

I am interested in how factors associated with mother can affect baby’s
birthweight, so I will choose  
`delwt:`(mother’s weight at delivery,  
`mheigth:` mother’s height (inches),  
`momage:` mother’s age at delivery (years), `smoken:` average number of
cigarettes smoked per day during pregnancy as predictors to build linear
regression model.

``` r
my_model = bw_df %>% 
  lm(bwt ~ delwt + mheight + momage + smoken, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable()
my_model
```

| term        |   estimate |   std.error |  statistic | p.value |
| :---------- | ---------: | ----------: | ---------: | ------: |
| (Intercept) | 978.977072 | 178.6454819 |   5.479999 |   0e+00 |
| delwt       |   0.012720 |   0.0007991 |  15.918648 |   0e+00 |
| mheight     |  15.969731 |   3.0413201 |   5.250921 |   2e-07 |
| momage      |  15.304540 |   1.9073020 |   8.024183 |   0e+00 |
| smoken      | \-7.023537 |   0.9941971 | \-7.064532 |   0e+00 |

  - The results show a positive intercept of 978.977072, and the
    parameters of delwt, mheight, momage and smoke are 0.012720,
    15.969731, 15.304540, -7.023537 respectively. The effect of these
    four predictors on birthweight are significant, since the P-value of
    all predictors are much less than 0.05.

**model residuals against fitted values**

``` r
my_model = bw_df %>% 
  lm(bwt ~ delwt + mheight + momage + smoken, data = .) 
plot = bw_df %>% 
  add_predictions(my_model) %>% 
  add_residuals(my_model) %>% 
  rename("prediction" = pred, "residual" = resid) %>%
  ggplot(aes(x = prediction, y = residual)) +
    geom_point(size = 0.9, alpha = 0.5, color = "pink") +
    labs(title = "Plot of Residual vs prediction", x="Predicted/Fitted value(grams)", y="Residual(grames)") + 
    geom_abline(intercept = 0, slope = 0)
plot
```

<img src="p8105_hw6_cz2634_files/figure-gfm/unnamed-chunk-5-1.png" width="90%" style="display: block; margin: auto;" />
\* we can not see obvious relationship between the residuals and the
predicted values from the plot above.

**Compare the three models**

  - model1 using length at birth and gestational age as predictors (main
    effects only)

<!-- end list -->

``` r
model1 = lm(bwt ~ blength + gaweeks, data = bw_df) %>% broom::tidy() %>% 
  knitr::kable()
model1
```

| term        |     estimate | std.error |  statistic | p.value |
| :---------- | -----------: | --------: | ---------: | ------: |
| (Intercept) | \-4347.66707 | 97.958360 | \-44.38281 |       0 |
| blength     |    128.55569 |  1.989891 |   64.60439 |       0 |
| gaweeks     |     27.04673 |  1.717930 |   15.74379 |       0 |

  - model2 using head circumference, length, sex, and all interactions
    (including the three-way interaction) between these

<!-- end list -->

``` r
model2 = lm(bwt ~ bhead * blength * babysex, data = bw_df) %>% broom::tidy() %>% 
  knitr::kable()
model2
```

| term                        |       estimate |    std.error |   statistic |   p.value |
| :-------------------------- | -------------: | -----------: | ----------: | --------: |
| (Intercept)                 | \-7176.8170222 | 1264.8397394 | \-5.6740920 | 0.0000000 |
| bhead                       |    181.7956350 |   38.0542051 |   4.7772811 | 0.0000018 |
| blength                     |    102.1269235 |   26.2118095 |   3.8962180 | 0.0000992 |
| babysexfemale               |   6374.8683508 | 1677.7669213 |   3.7996150 | 0.0001469 |
| bhead:blength               |    \-0.5536096 |    0.7802092 | \-0.7095656 | 0.4780117 |
| bhead:babysexfemale         |  \-198.3931810 |   51.0916850 | \-3.8830816 | 0.0001047 |
| blength:babysexfemale       |  \-123.7728875 |   35.1185360 | \-3.5244319 | 0.0004288 |
| bhead:blength:babysexfemale |      3.8780531 |    1.0566296 |   3.6702106 | 0.0002453 |

**cross validation**

``` r
#crossv_mc() will get 500 repeated sampling, each sample has sample size as the original dataset, and create train and test data
cv_df = 
  crossv_mc(bw_df, 500) %>% 
    mutate(
        train = map(train, as.tibble),
        test = map(test,as.tibble)
    )  %>%
  
  mutate(my_model  = map(train, ~lm(bwt ~ delwt + mheight + momage + smoken, data = .x)),
         model1  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         model2  = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  
  mutate(rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)),
         rmse_model1 = map2_dbl(model1 , test, ~rmse(model = .x, data = .y)),
         rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y))) 
cv_df
## # A tibble: 500 x 9
##    train test  .id   my_model model1 model2 rmse_my_model rmse_model1
##    <lis> <lis> <chr> <list>   <list> <list>         <dbl>       <dbl>
##  1 <tib~ <tib~ 001   <lm>     <lm>   <lm>            491.        330.
##  2 <tib~ <tib~ 002   <lm>     <lm>   <lm>            477.        327.
##  3 <tib~ <tib~ 003   <lm>     <lm>   <lm>            505.        336.
##  4 <tib~ <tib~ 004   <lm>     <lm>   <lm>            480.        339.
##  5 <tib~ <tib~ 005   <lm>     <lm>   <lm>            489.        320.
##  6 <tib~ <tib~ 006   <lm>     <lm>   <lm>            489.        359.
##  7 <tib~ <tib~ 007   <lm>     <lm>   <lm>            490.        363.
##  8 <tib~ <tib~ 008   <lm>     <lm>   <lm>            472.        320.
##  9 <tib~ <tib~ 009   <lm>     <lm>   <lm>            470.        348.
## 10 <tib~ <tib~ 010   <lm>     <lm>   <lm>            488.        339.
## # ... with 490 more rows, and 1 more variable: rmse_model2 <dbl>
```

``` r
#select interested variables 
rmse_df = cv_df %>% 
select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>%   mutate(
    model = fct_reorder(model,rmse))
rmse_df
## # A tibble: 1,500 x 2
##    model     rmse
##    <fct>    <dbl>
##  1 my_model  491.
##  2 model1    330.
##  3 model2    297.
##  4 my_model  477.
##  5 model1    327.
##  6 model2    291.
##  7 my_model  505.
##  8 model1    336.
##  9 model2    298.
## 10 my_model  480.
## # ... with 1,490 more rows
```

``` r
plot2 = rmse_df %>% 
  ggplot(aes(x = model, y = rmse)) + 
    geom_violin(aes(fill = model), color = "black", alpha = .5) +
    labs(title = "Comparisions of three models", y="RMSE value", x = "Models") 
plot2
```

<img src="p8105_hw6_cz2634_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" style="display: block; margin: auto;" />
From the plot, we can observe that `my_model` has greater RMSE than the
other two models, and `model2` has smaller RMSE than `model1`. The
results peove that prediction accuracy of `model2` is higher than the
other two. The prediction accuracy of `my_model` is too low to be a good
linear model. ter than `model1`. I would recommand use `model2` for
hypothetical analysis, or find a better model with higher prediction
accuracy.

## Problem 3

**import data**

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

**bootstrapping** \* use bootstrap to get repeated sampling framework  
\* focus on a simple linear regression with tmax as the response and
tmin as the predictor

``` r
bootstrap_weather = 
  weather_df %>%
  bootstrap(n = 5000) %>%
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::glance),
    results2 = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results, results2) %>% 
  select(.id, term, r.squared,estimate) %>% 
  mutate(
    term = str_replace(term, "\\(", ""),
    term = str_replace(term, "\\)", "")
  )
bootstrap_weather
## # A tibble: 10,000 x 4
##    .id   term      r.squared estimate
##    <chr> <chr>         <dbl>    <dbl>
##  1 0001  Intercept     0.911     7.13
##  2 0001  tmin          0.911     1.05
##  3 0002  Intercept     0.920     7.66
##  4 0002  tmin          0.920     1.01
##  5 0003  Intercept     0.927     6.87
##  6 0003  tmin          0.927     1.06
##  7 0004  Intercept     0.908     7.31
##  8 0004  tmin          0.908     1.04
##  9 0005  Intercept     0.926     7.44
## 10 0005  tmin          0.926     1.03
## # ... with 9,990 more rows
```

``` r
bootstrap_weather2 = bootstrap_weather %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>%
  mutate(log = log(tmin * Intercept, base = exp(1))) %>%
  select(r.squared, log)

bootstrap_weather2
## # A tibble: 5,000 x 2
##    r.squared   log
##        <dbl> <dbl>
##  1     0.911  2.01
##  2     0.920  2.05
##  3     0.927  1.99
##  4     0.908  2.03
##  5     0.926  2.03
##  6     0.908  1.99
##  7     0.907  2.00
##  8     0.906  2.01
##  9     0.926  2.01
## 10     0.909  2.01
## # ... with 4,990 more rows
```

\*\*Construct bootstrap CI for r squared and log(β0\*β1)\*\*

``` r
bootstrap_weather2 %>% 
  summarize(
    r_ci_lower = quantile(r.squared, 0.025),
    r_ci_upper = quantile(r.squared, 0.975)
  ) %>% 
  knitr::kable(caption = "95% CI for r square")
```

| r\_ci\_lower | r\_ci\_upper |
| -----------: | -----------: |
|    0.8937488 |    0.9272957 |

95% CI for r square

``` r
bootstrap_weather2 %>% 
  summarize(
    log_ci_lower = quantile(log, 0.025),
    log_ci_upper = quantile(log, 0.975)
  ) %>% 
  knitr::kable(caption = "95% CI for r square")
```

| log\_ci\_lower | log\_ci\_upper |
| -------------: | -------------: |
|        1.96644 |       2.058637 |

95% CI for r square

\*\*Ditribution of r squared and log(β0\*β1)\*\*

``` r
plot3 = bootstrap_weather2 %>% 
  ggplot(aes(x = r.squared)) +
    geom_density(alpha = 0.5, fill = "pink", color = "black" ) +
    labs(title = "Distribution of r.squared", x="r.squared", y="Density") 
plot3
```

<img src="p8105_hw6_cz2634_files/figure-gfm/unnamed-chunk-16-1.png" width="90%" style="display: block; margin: auto;" />

  - The plot shows the distribution of r\_squared. It looks like normal
    distribution. But We can observe a left tail. What is more, the
    curve is not as smooth as normal distribution, especially in the
    left part. The mean of r\_squared is 0.91 and the standard deviation
    is 0.01.

<!-- end list -->

``` r
plot4 = bootstrap_weather2 %>% 
  ggplot(aes(x = log)) +
    geom_density(alpha = 0.5, fill = "pink", color = "black" ) +
    labs(title = "Distribution of log(β0*β1)", x="log(β0*β1)", y="Density") 
plot4
```

<img src="p8105_hw6_cz2634_files/figure-gfm/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;" />

  - This curve also look like normal distribution. But it has a more
    flat peak than usual normal distibution and two long tails on both
    sides. The mean is 2.013073, and the statdard deviation is 0.023634
